<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>Zeyan LI&#39;s Homepage</title><meta name="keywords" content="Zeyan LI&#39;s Homepage"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:type" content="website"><meta property="og:title" content="Zeyan LI&#39;s Homepage"><meta property="og:url" content="https://blog.lizeyan.me/index.md/page/2/index.html"><meta property="og:site_name" content="Zeyan LI&#39;s Homepage"><meta property="og:locale" content="en"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Zeyan LI&#39;s Homepage"><link rel="icon" href="/img/chitanta.png"><link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/libs/open-sans/styles.css"><link rel="stylesheet" href="/libs/source-code-pro/styles.css"><link rel="stylesheet" href="/css/style.css"><script src="/libs/jquery/2.1.3/jquery.min.js"></script><script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script><link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css"><link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css"><script type="text/javascript">!function(e,a,t,n,g,c,o){e.GoogleAnalyticsObject="ga",e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),o=a.getElementsByTagName(t)[0],c.async=1,c.src="//www.google-analytics.com/analytics.js",o.parentNode.insertBefore(c,o)}(window,document,"script"),ga("create","UA-138651056-1","auto"),ga("send","pageview")</script></head></html><body><div id="container"><header id="header"><div id="header-main" class="header-inner"><div class="outer"><a href="/" id="logo"><span class="site-title">Zeyan LI&#39;s Homepage</span></a><nav id="main-nav"><a class="main-nav-link" href="/">Home</a> <a class="main-nav-link" href="/archives">Archives</a> <a class="main-nav-link" href="/categories">Categories</a> <a class="main-nav-link" href="/tags">Tags</a> <a class="main-nav-link" href="/">About</a></nav><nav id="sub-nav"><div class="profile" id="profile-nav"><a id="profile-anchor" href="javascript:;"><img class="avatar" src="https://www.gravatar.com/avatar/07d6dce3cbd35d7c566b5f53f05dba2d"> <i class="fa fa-caret-down"></i></a></div></nav><div id="search-form-wrap"><form class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"> <button type="submit" class="search-form-submit"></button></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="Type something..."> <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"/",CONTENT_URL:"/content.json"}</script><script src="/js/insight.js"></script></div></div></div><div id="main-nav-mobile" class="header-sub header-inner"><table class="menu outer"><tr><td><a class="main-nav-link" href="/">Home</a></td><td><a class="main-nav-link" href="/archives">Archives</a></td><td><a class="main-nav-link" href="/categories">Categories</a></td><td><a class="main-nav-link" href="/tags">Tags</a></td><td><a class="main-nav-link" href="/">About</a></td><td><div class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"></div></td></tr></table></div></header><div class="outer"><aside id="profile"><div class="inner profile-inner"><div class="base-info profile-block"><img id="avatar" src="https://www.gravatar.com/avatar/07d6dce3cbd35d7c566b5f53f05dba2d?s=128"><h2 id="name">Zeyan LI</h2><h3 id="title">Ph.D. student</h3><span id="location"><i class="fa fa-map-marker"></i>Beijing, China</span> <a id="follow" target="_blank" href="https://github.com/lizeyan/">FOLLOW</a></div><div class="article-info profile-block"><div class="article-info-block">38 <span>posts</span></div><div class="article-info-block">3 <span>tags</span></div></div><div class="profile-block social-links"><table><tr><td><a href="https://github.com/lizeyan/" target="_blank" title="github" class="tooltip"><i class="fa fa-github"></i></a></td></tr></table></div></div></aside><aside id="sidebar"><div class="widget-wrap" id="categories"><h3 class="widget-title"><span>categories</span> &nbsp; <a id="allExpand" href="#"><i class="fa fa-angle-double-down fa-2x"></i></a></h3><ul class="unstyled" id="tree"><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Algorithm</a><ul class="unstyled" id="tree"><li class="file"><a href="/Algorithm/PC_algorithm/">PC Algorithm</a></li><li class="file"><a href="/Algorithm/association_rule_mining/">Association Rule Mining</a></li><li class="file"><a href="/Algorithm/locality_sensitive_hash/">Locality Sensitive Hashing</a></li><li class="file"><a href="/Algorithm/pagerank/">Page Rank</a></li><li class="file"><a href="/Algorithm/sequential_pattern_mining/">Sequential Pattern Mining</a></li><li class="file"><a href="/Algorithm/tsne/">t-SNE</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Conference</a><ul class="unstyled" id="tree"><li class="file"><a href="/Conference/ISSRE/"></a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; DeepLearningBook</a><ul class="unstyled" id="tree"><li class="file"><a href="/DeepLearningBook/optimization/">Optimization</a></li><li class="file"><a href="/DeepLearningBook/regularization/">Regularization</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Dreaming Dogs</a><ul class="unstyled" id="tree"><li class="file"><a href="/Dreaming Dogs/What is Dreaming Dogs/">Dreaming Dogs</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Latex</a><ul class="unstyled" id="tree"><li class="file"><a href="/Latex/cjk/">cjk</a></li><li class="file"><a href="/Latex/latex_snippets/">LaTex Snippets</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Linux</a><ul class="unstyled" id="tree"><li class="file"><a href="/Linux/bash_redirect_output/">Bash Redirect Output</a></li><li class="file"><a href="/Linux/ubuntu_1804_install_notes/">Ubuntu 18.04 install notes</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; PaperNotes</a><ul class="unstyled" id="tree"><li class="file"><a href="/PaperNotes/gan/">GAN</a></li><li class="file"><a href="/PaperNotes/gan_anomaly_detection/">GAN Anomaly Detection</a></li><li class="file"><a href="/PaperNotes/network_diffusion/">Network Diffusion</a></li><li class="file"><a href="/PaperNotes/train_gan/">Train GAN</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Proxmox Virtual Environment</a><ul class="unstyled" id="tree"><li class="file"><a href="/Proxmox Virtual Environment/unable_to_access_host/">Unable to Access Host via PVE API</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Python</a><ul class="unstyled" id="tree"><li class="file"><a href="/Python/Einstein-Summation-Convention/">Einstein Summation Convention</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; StatisticalMachineLearning</a><ul class="unstyled" id="tree"><li class="file"><a href="/StatisticalMachineLearning/boost/">boost</a></li><li class="file"><a href="/StatisticalMachineLearning/clustering/">Clustering</a></li><li class="file"><a href="/StatisticalMachineLearning/dimension_reduction/">Dimension Reduction</a></li><li class="file"><a href="/StatisticalMachineLearning/kernel_density_estimation/">Kernel Density Estimation</a></li><li class="file"><a href="/StatisticalMachineLearning/nb_and_lr/">NB and LR</a></li><li class="file"><a href="/StatisticalMachineLearning/reinforcement_learning/">Reinforcement Learning</a></li><li class="file"><a href="/StatisticalMachineLearning/svm/">SVM</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Statistics</a><ul class="unstyled" id="tree"><li class="file"><a href="/Statistics/test/">Statistical Test</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; configuration</a><ul class="unstyled" id="tree"><li class="file"><a href="/configuration/hexo_config/">Hexo Config</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; macOS</a><ul class="unstyled" id="tree"><li class="file"><a href="/macOS/install_homebrew/">Install Homebrew</a></li><li class="file"><a href="/macOS/install_supervisor/">Install supervisor via brew</a></li><li class="file"><a href="/macOS/macos_unable_to_fork_issue/">macOS Catalina unable to fork issue</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; nginx</a><ul class="unstyled" id="tree"><li class="file"><a href="/nginx/log_format/">log format of nginx</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; pypy</a><ul class="unstyled" id="tree"><li class="file"><a href="/pypy/install_pypy/">install PyPy</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; seafile</a><ul class="unstyled" id="tree"><li class="file"><a href="/seafile/client_conflicts/">Fix 'there is a conflict with an existing library'</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; 日本語</a><ul class="unstyled" id="tree"><li class="file"><a href="/日本語/キセキ/">キセキ</a></li><li class="file"><a href="/日本語/本文/">本文</a></li><li class="file"><a href="/日本語/用言/">用言</a></li></ul></li></ul></div><script>$(document).ready(function(){var r="fa-folder-open",i="fa-folder",l="fa-angle-double-down",d="fa-angle-double-up";$(document).on("click",'#categories a[data-role="directory"]',function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(r),l=$(this).siblings("ul");e.removeClass(r).removeClass(i),s?(void 0!==l&&l.slideUp({duration:100}),e.addClass(i)):(void 0!==l&&l.slideDown({duration:100}),e.addClass(r))}),$('#categories a[data-role="directory"]').bind("contextmenu",function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(r),l=$(this).siblings("ul"),d=$.merge(l.find("li ul"),l),o=$.merge(l.find(".fa"),e);o.removeClass(r).removeClass(i),s?(d.slideUp({duration:100}),o.addClass(i)):(d.slideDown({duration:100}),o.addClass(r))}),$(document).on("click","#allExpand",function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(l);e.removeClass(l).removeClass(d),s?($("#sidebar .fa.fa-folder").removeClass("fa-folder").addClass("fa-folder-open"),$("#categories li ul").slideDown({duration:100}),e.addClass(d)):($("#sidebar .fa.fa-folder-open").removeClass("fa-folder-open").addClass("fa-folder"),$("#categories li ul").slideUp({duration:100}),e.addClass(l))})})</script><div id="toTop" class="fa fa-angle-up"></div></aside><section id="main"><article id="post-StatisticalMachineLearning/reinforcement_learning" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/StatisticalMachineLearning/">StatisticalMachineLearning</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/StatisticalMachineLearning/reinforcement_learning/"><time datetime="2020-03-27T09:10:54.108Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/StatisticalMachineLearning/reinforcement_learning.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/StatisticalMachineLearning/reinforcement_learning.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/StatisticalMachineLearning/reinforcement_learning.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/StatisticalMachineLearning/reinforcement_learning/">Reinforcement Learning</a></h1></header><div class="article-entry" itemprop="articleBody"><p>[TOC]</p><h2 id="introduction">Introduction</h2><ul><li><p>Agent:</p><p>At each step <span class="math inline">\(t\)</span>, the agent</p><ul><li>Receives state <span class="math inline">\(s_t\)</span></li><li>Receives scalar reward <span class="math inline">\(r_t\)</span></li><li>Executes action <span class="math inline">\(a_t\)</span></li></ul><p>Agent's goal is learn a policy to maximize long-term total reward:</p><p><span class="math inline">\(\sum_{t=1}^{T}r_t\)</span> or <span class="math inline">\(\sum_{t=1}^{\infty}\gamma^tr_t\)</span></p></li><li><p>Environment</p><ul><li>Receives action <span class="math inline">\(a_t\)</span></li><li>Emits state <span class="math inline">\(s_t\)</span></li><li>Emits scalar reward <span class="math inline">\(r_t\)</span></li></ul></li><li><p>Policy</p><p>Agent's behavior <span class="math display">\[ a=\pi(s) \]</span> or <span class="math display">\[ \pi(a|s)=\mathbb{P}[A_t=a|S_t=s] \]</span></p></li><li><p>Value Function</p><p>the prediction of future reward given a state and an action</p><p>Q-value function gives expected total reward <span class="math display">\[ Q^{\pi}(s, a)=\mathbb{E}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+...|s,a] \]</span> It can be decompose into a Bellman equation <span class="math display">\[ Q^{\pi}(s, a)=\mathbb{E}_{s&#39;,a&#39;}[r+\gamma Q^\pi(s&#39;,a&#39;)|s,a] \]</span></p></li><li><p>Model</p><p>A model predicts what the environment will do next <span class="math display">\[ \mathcal{P}_{ss&#39;}^a=\mathbb{P}[S_{t+1}=s&#39;|S_t=s,A_t=a]\\ \mathcal{R}_s^a=\mathbb{E}[R_{t+1}|S_t=s,A_t=a] \]</span></p></li></ul><h2 id="markov-decision-process">Markov Decision Process</h2><p>History is the sequence of observations ,actions, rewards <span class="math display">\[ H_t=O_1,R_1,A_1,...,A_{t-1},O_t,R_t \]</span> Formmaly, state is a function of the history <span class="math display">\[ S_t=f(H_t) \]</span> A state <span class="math inline">\(S_t\)</span> is Markov iff <span class="math display">\[ \mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1,...,S_t] \]</span> The state is a sufficient statictic of the future. Once you get the state, you can throw the history away.</p><p>A Markov Decision Process is a tuple <span class="math inline">\(&lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma&gt;\)</span></p><p><span class="math inline">\(\mathcal S\)</span> is a finite set of states.</p><p><span class="math inline">\(\mathcal A\)</span> is a finite set of actions</p><p><span class="math inline">\(\mathcal P\)</span> is a state transition probability matrix</p><p><span class="math inline">\(\mathcal R\)</span> is a reward function</p><p><span class="math inline">\(\gamma\)</span> is a discount factor</p><p>How to solve the optimal policy in MDP?</p><ul><li>Value-Based RL</li><li>Policy-Based RL</li><li>Molde-Based RL</li></ul><p>State value function <span class="math inline">\(V^{\pi}(s)=E[\sum_{t=1}^{T}r_t|s]\)</span></p><p>State-action value function <span class="math inline">\(Q^{\pi}(s,a)=E[\sum_{t=1}^{T}r_t|s,a]=\sum_{s&#39;}P(s&#39;|s,a)(R(s,a,s&#39;)+V^{\pi}(s&#39;))\)</span> <span class="math display">\[ V^{\pi}(s)=\sum_a\pi(a|s)Q(s,a) \]</span></p><h2 id="value-based-methods">Value Based Methods</h2><h2 id="policy-search">Policy Search</h2><h2 id="model-based-method">Model-Based Method</h2><h2 id="deep-reinforcement-learning">Deep Reinforcement Learning</h2></div><footer class="article-footer"></footer></div></article><article id="post-StatisticalMachineLearning/svm" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/StatisticalMachineLearning/">StatisticalMachineLearning</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/StatisticalMachineLearning/svm/"><time datetime="2020-03-27T09:10:54.108Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/StatisticalMachineLearning/svm.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/StatisticalMachineLearning/svm.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/StatisticalMachineLearning/svm.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/StatisticalMachineLearning/svm/">SVM</a></h1></header><div class="article-entry" itemprop="articleBody"><p></p><p>SVM</p><p></p></div><div class="article-more-link"><a href="/StatisticalMachineLearning/svm/#more">Read More</a></div><footer class="article-footer"></footer></div></article><article id="post-StatisticalMachineLearning/nb_and_lr" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/StatisticalMachineLearning/">StatisticalMachineLearning</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/StatisticalMachineLearning/nb_and_lr/"><time datetime="2020-03-27T09:10:54.096Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/StatisticalMachineLearning/nb_and_lr.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/StatisticalMachineLearning/nb_and_lr.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/StatisticalMachineLearning/nb_and_lr.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/StatisticalMachineLearning/nb_and_lr/">NB and LR</a></h1></header><div class="article-entry" itemprop="articleBody"><p></p><p>Naive Bayesian and Logistic Regression</p><p></p></div><div class="article-more-link"><a href="/StatisticalMachineLearning/nb_and_lr/#more">Read More</a></div><footer class="article-footer"></footer></div></article><article id="post-StatisticalMachineLearning/dimension_reduction" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/StatisticalMachineLearning/">StatisticalMachineLearning</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/StatisticalMachineLearning/dimension_reduction/"><time datetime="2020-03-27T09:10:54.092Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/StatisticalMachineLearning/dimension_reduction.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/StatisticalMachineLearning/dimension_reduction.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/StatisticalMachineLearning/dimension_reduction.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/StatisticalMachineLearning/dimension_reduction/">Dimension Reduction</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="pca">PCA</h2><h3 id="algorithm">Algorithm</h3>$$ preamble \newcommand{\vv}[1]{\boldsymbol{#1}} $$<p>Given data matrix <span class="math inline">\(\mathbf{X}\)</span>, get the <span class="math inline">\(d\)</span>-largest eigenvalues <span class="math inline">\(\lambda_1, ..., \lambda_d\)</span> and correbounding egivenvectors <span class="math inline">\(\vv{u}_1, \vv{u}_2, ..., \vv{u}_d\)</span></p><p>let <span class="math inline">\(\mathbf{U}=\begin{bmatrix}\vv u_1, \vv u_2,..., \vv u_d\end{bmatrix}\)</span></p><p>Encode: <span class="math inline">\(\mathbf{U}^\top \mathbf X\)</span></p><p>Decode: $ Z$</p><p>​</p><h3 id="maximum-variance-formulation">Maximum Variance Formulation</h3><h4 id="d-case">1-d case</h4><p>The projection direction <span class="math inline">\(\vv{u}\)</span> satisfies <span class="math inline">\(||\vv{u}||=1\)</span> <span class="math display">\[ y=\vv{u}^\top\vv{x}\\ \bar{\vv{y}}=\vv{u}^\top\bar{\vv{x}}\\ var(\vv{y})=\frac{1}{N}\sum_{i=1}^{N}(\vv u^\top\vv{x}_n-\vv{u}^\top\bar{\vv{x}})^2\\ =\vv{u}^\top \mathbf{S} \vv{u}, \mathbf{S}=\frac{1}{N}(\vv{x}_n-\bar{\vv{x}})(\vv{x}_n-\bar{\vv{x}})^\top \]</span></p><p><span class="math display">\[ \hat{\vv{u}}=\text{argmax}_\vv{u}\vv{u}^\top \mathbf{S} \vv{u}\\ s.t.\&gt;\&gt; \vv{u}^\top\vv{u}=1 \]</span></p><p>Solve it, we get <span class="math display">\[ \mathbf{S}\vv{u}=\lambda\vv{u}\\ \lambda=\vv{u}^\top \mathbf{S} \vv{u} \]</span> Therefore <span class="math inline">\(\lambda\)</span> is the largest eigen-value.</p><h4 id="add-more-component">add more component</h4><p><span class="math display">\[ \hat{\vv{u}}_2=\text{argmax}_\vv{u}\vv{u}^\top \mathbf{S} \vv{u}\\ s.t.\&gt;\&gt; \vv{u}^\top\vv{u}=1 \\ \&gt;\&gt; \vv{u}_1^\top\vv{u}=0 \]</span></p><p><span class="math display">\[ \mathbf{S} \vv u_2 - \lambda \vv u_2 - \gamma \vv u_1=0\\ \vv u_1^\top \mathbf{S} \vv u_2 - 0 - \gamma = 0\\ \mathbf{S} \vv u_2 - \lambda \vv u_2=0\\ \lambda=\vv u_2^\top \mathbf{S} \vv u_2 \]</span></p><p><span class="math display">\[ \hat{\vv\mu_3}=\text{argmax}_{\mu_3}\vv\mu_3^\top\mathbf{S}\vv\mu_3\\ s.t. \&gt;\&gt; \vv{\mu_3}^\top\vv\mu_3=1, \vv\mu_3^\top\vv\mu_2=0, \vv\mu_3^\top\vv\mu_1=0,\vv\mu_2^\top\vv\mu_1=0 \]</span></p><p><span class="math display">\[ \mathbf S\vv\mu_3-\lambda \vv\mu_3-\gamma_2\vv\mu_2-\gamma_1\vv\mu_1=0\\ \vv\mu_1^\top\mathbf{S}\vv\mu_3-0-0-\gamma_1=0\\ \vv\mu_2^\top\mathbf{S}\vv\mu_3-0-\gamma_2-0=0\\ \therefore \gamma_1=\gamma_2=0\\ \therefore \mathbf{S}\vv\mu_3=\lambda \vv\mu_3 \\ \therefore \vv\mu_3^\top\mathbf{S}\vv\mu_3=\lambda \]</span></p><p>Therefore <span class="math inline">\(\vv\mu_2, \vv\mu_3\)</span> must be the eigenvector with 2nd and 3rd largest engienvalues.</p><h3 id="minimum-error-formulation">Minimum Error Formulation</h3><p>A set of complete orthonormal basis <span class="math display">\[ \{\vv u_1,\vv u_2, ..., \vv u_n\} \]</span> Then <span class="math inline">\(\vv x\)</span> can be represented by <span class="math inline">\(\vv{x}=\sum_i \alpha_i \vv u_i, \alpha_i=\vv{x}^\top\vv u_i\)</span></p><p>Consider a low-dimension representation: <span class="math display">\[ \vv{x}_n=\sum_{i=1}^{d}z_{ni}\vv u_i + \sum_{i=d+1}^{D}b_i\vv u_i \]</span></p><p><span class="math display">\[ J=\frac{1}{N}\sum_{n=1}^{N}||x_n-\hat x_n||^2 \]</span></p><p><span class="math display">\[ \frac{dJ}{dz_{ni}}=2(x_n-\sum_{i=1}^dz_{ni}u_i-\sum_{i=d+1}^Db_iu_i)^\top u_i=0\\ x_n^\top u_i -z_{ni}=0 \]</span></p><p><span class="math display">\[ \frac{dJ}{db_i}=2\sum_{n=1}^N(x_n-\sum_{i=1}^dz_{ni}u_i-\sum_{i=d+1}^Db_iu_i)^\top u_i=0\\ \sum_{n=1}^{N}x_n^\top u_i-N b_i=0 \]</span></p><p>Therefore the error <span class="math inline">\(J\)</span> equals <span class="math display">\[ J=\sum_{n=1}^{N}\sum_{i=d+1}^{D}((x_i-\bar x)^\top u_i)u_i\\ =\sum_{i=d+1}^{D}u_i^\top S u_i \]</span> It can be prove similarly that <span class="math inline">\(u_i\)</span> should be the <span class="math inline">\(D-d\)</span> smallest eigen-values' eigen-vectors. <span class="math display">\[ \hat x_n = \sum_{i=1}^{d}(x_n^\top u_i + \bar x ^\top u_i - \bar x ^\top u_i)u_i + \sum_{i=d+1}^{D}(\bar x^\top u_i)u_i\\ =\bar x + \sum_{i=1}^{d}(x_n-\bar x)^\top u_i u_i \]</span> <img src="/StatisticalMachineLearning/dimension_reduction/image-20190419195806342.png" alt="image-20190419195806342"></p><h2 id="probabilistic-pca">Probabilistic PCA</h2><p>Let <span class="math inline">\(z\)</span> be a latent feature vector. We assume its prior <span class="math inline">\(z\sim N(0, I)\)</span></p><p>Assume that <span class="math inline">\(x=Wz+\mu+\epsilon, \epsilon\sim N(0, \sigma^2 I)\)</span></p><p>Therefore <span class="math display">\[ p(x|z)=N(x|Wz+\mu, \sigma^2I)\\ p(x)=\int_zp(x|z)p(z)dz=N(x|\mu, C=WW^\top+\sigma^2I) \]</span></p><p><span class="math display">\[ C^{-1}=\sigma^{-2} I-\sigma^{-2}WM^{-1}W^\top\\ M=W^\top W+\sigma^2I \]</span></p><p><span class="math display">\[ p(z|x)=\mathcal N(z|M^{-1}W^\top(x-u), \sigma^{-2}M) \]</span></p><p>Apply MLE on <span class="math inline">\(p(x)\)</span> <span class="math display">\[ \nabla_{u}p(x)=\nabla_{u}-\frac{Np}{2}\log(2\pi)-\frac{N}{2}\log|C|-\frac{1}{2}\sum_{n=1}^{N}(x_n-\mu)^\top C^{-1}(x_n-\mu)\\ =C^{-1}\sum_{n=1}^{N}(x_n-\mu)=0\\ \therefore \mu=\bar x \]</span></p><p><span class="math display">\[ \log p(x)=-\frac{N}{2}(p\log(2\pi)+\log|C|+tr(C^{-1}S)) \]</span></p><figure><img src="/StatisticalMachineLearning/dimension_reduction/image-20190423153437444.png" alt="image-20190423153437444"><figcaption>image-20190423153437444</figcaption></figure><p>We can choose <span class="math inline">\(R=I\)</span></p><p>Let <span class="math inline">\(\sigma^2\to 0\)</span> <span class="math display">\[ \mathbb{E}[z|x]=M^{-1}W^\top(x-\mu)\\ =(W^\top W)^{-1}W(x-\mu)=W(x-\mu) \]</span></p></div><footer class="article-footer"></footer></div></article><article id="post-StatisticalMachineLearning/kernel_density_estimation" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/StatisticalMachineLearning/">StatisticalMachineLearning</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/StatisticalMachineLearning/kernel_density_estimation/"><time datetime="2020-03-27T09:10:54.092Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/StatisticalMachineLearning/kernel_density_estimation.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/StatisticalMachineLearning/kernel_density_estimation.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/StatisticalMachineLearning/kernel_density_estimation.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/StatisticalMachineLearning/kernel_density_estimation/">Kernel Density Estimation</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="kde">KDE</h2><p>Kernel density estimation (KDE) is a non-parametric way to estimate the probability distribution function (PDF) of a random variable.</p><p>若 <span class="math inline">\(\{x_n, n=1,2,...n\}\)</span> 为一列<span class="math inline">\(iid\)</span>的样本，那么它的KDE是 <span class="math display">\[ \hat f_h(x)=\frac{1}{nh}\sum_{i=1}^{n}K(\frac{x-x_i}{h}) \]</span> <span class="math inline">\(h\)</span>是bandwith，对KDE的结果有很大影响</p><p><a href="https://github.com/lizeyan/lizeyan.github.io/blob/master/jupyter-notebooks/kernel-density-estimation.ipynb" target="_blank" rel="noopener">KDE示例</a></p><h2 id="bandwidth的选择">Bandwidth的选择</h2></div><footer class="article-footer"></footer></div></article><article id="post-StatisticalMachineLearning/clustering" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/StatisticalMachineLearning/">StatisticalMachineLearning</a></div><div class="article-tag"><i class="fa fa-tag"></i> <a class="tag-link" href="/tags/unsupervised/">unsupervised</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/StatisticalMachineLearning/clustering/"><time datetime="2020-03-27T09:10:54.088Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/StatisticalMachineLearning/clustering.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/StatisticalMachineLearning/clustering.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/StatisticalMachineLearning/clustering.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/StatisticalMachineLearning/clustering/">Clustering</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="clustering">Clustering</h2><p>cluster means group objects into classes of similar objects</p><ul><li>Minimize inter-class similarity</li><li>Maximize intra-class similarity</li></ul><h3 id="metric-sapce">Metric Sapce</h3><p>What is distance?</p><ol type="1"><li><span class="math inline">\(d(x, y) = d(y, x)\)</span></li><li><span class="math inline">\(d(x , y) \ge 0\)</span> and <span class="math inline">\(d(x, y)=0 \Leftrightarrow x = y\)</span></li><li><span class="math inline">\(d(x,y)\le d(x, z) + d(z, y)\)</span></li></ol><p>Examples:</p><ul><li>(distance derived from) <span class="math inline">\(p\)</span>-norm</li><li>edit distance</li><li>hamming distance</li><li>Cosine distance</li><li>Non-metric distances, e.g. DTW, perceptual loss</li></ul><h3 id="k-means">K-Means</h3><h4 id="algorithm">Algorithm</h4><ol type="1"><li>Initialize <span class="math inline">\(\mu_1, ..., \mu_K\)</span></li><li>Repeat until no change happens<ol type="1"><li>Expectation: for each <span class="math inline">\(k\)</span>, <span class="math inline">\(C_k=\{i\&gt;s.t.\&gt; x_i\text{ is closest to }\mu_k\}\)</span></li><li>Maximization: for each <span class="math inline">\(k\)</span>, update <span class="math inline">\(\mu_k=\frac{1}{|C_k|}\sum_{i\in C_k}x_i\)</span></li></ol></li></ol><h4 id="optimization-problem">Optimization problem</h4><p><span class="math display">\[ J=\sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}||x_n-\mu_k||^2 \\ s.t. \&gt;\&gt; \sum_{k=1}^{K}r_{nk}=1, r_{nk}\in\{0, 1\}\\ \mu_k=\frac{\sum_ix_i1_{r_{nk}=1}}{\sum_i1_{r_{nk}=1}} \]</span></p><p>In each expection step, we keep <span class="math inline">\(\mu_k\)</span> fixed and optimize <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(r_{nk}\)</span> It has closed form solution: <span class="math display">\[r_{nk}=\begin{cases}1 &amp; k=\text{argmin}_i ||x_n-\mu_i||^2 \\ 0 &amp; otherwise\end{cases}\]</span>​</p><p>In each maimization step, we keep <span class="math inline">\(r_{nk}\)</span> fixed and optimize <span class="math inline">\(J\)</span> with respect to <span class="math inline">\(\mu_k\)</span></p><p>It has closed form solution: <span class="math display">\[ \because \frac{\partial J}{\partial \mu_k}=\sum_i2 r_{ik} (x_i-\mu_k)=0\\ \therefore \mu_k=\frac{\sum_n x_nr_{nk}}{\sum_n r_{nk}} \]</span></p><h4 id="k-means-as-gradient-descent">K-Means as Gradient Descent</h4><p><img src="/StatisticalMachineLearning/clustering/image-20190419151655736.png" alt="image-20190419151655736">s</p><p>Gradient descent can be applied.</p><p>Second oder gradient descent leads to the same update rule as k-means.</p><h4 id="find-a-good-optimum">Find a Good Optimum</h4><p>K-means leads to a local optimum</p><ul><li>find a good start point</li><li>Run many times of k-means and choose a best one</li><li>....</li></ul><h3 id="gaussian-mixture">Gaussian Mixture</h3><h4 id="gaussian">Gaussian</h4><p>MLE for Gaussian leads to natual solutions</p><h4 id="gaussian-mixture-1">Gaussian Mixture</h4><p><span class="math display">\[ p(x)=\sum_{k=1}^{K}\pi_k\mathcal{N}(x|\mu_k, \Sigma_k)\\ p(x|z_k=1)=\mathcal{N}(x|\mu_k, \Sigma_k)\\ p(z)=\prod_{k=1}^{K}\pi_k^{z_k}, \pi_k\in[0, 1], \sum_{k=1}^{K}\pi_k=1 \]</span></p><p><span class="math display">\[ \mathcal{L}_0=\log p(D)\\ =\sum_{n=1}^{N}\log(\sum_{k=1}^{K}\pi_k\mathcal N(x|\mu_k, \Sigma_k)) \]</span></p><p>Here, we assume that <span class="math inline">\(\pi_k\)</span> uniformly dsitrbuted. However, we can use another prior like Dirichlet distribution. <span class="math display">\[ \mathcal{L}_0=\log p(D)\\ =\sum_{n=1}^{N}\log(\sum_{k=1}^{K}p(\pi_k)\pi_k\mathcal N(x|\mu_k, \Sigma_k)) \]</span></p><p>Because of the constraints, we should use Lagrange multiplier method: <span class="math display">\[ \mathcal{L}=\mathcal{L}_0+\lambda(\sum_{k=1}^{K}\pi_k-1) \]</span></p><p>Before do MLE, let's first take a look at the posterior: <span class="math display">\[ p(z_k=1|x)=\frac{p(x|z_k=1)p(z_k=1)}{p(x)}\\ =\frac{\pi_k\mathcal{N}(x|\mu_k, \Sigma_k)}{\sum_{i=1}^{K}\pi_i\mathcal{N}(x|\mu_i, \Sigma_i)} \]</span> Let <span class="math inline">\(\gamma(z_k)=p(z_k=1|x)\)</span></p><p>Then <span class="math display">\[ \because \frac{\partial \mathcal L}{\partial \mu_k} =-\sum_{n=1}^{N}\frac{\pi_k\mathcal N(x_n|\mu_k, \Sigma_k)}{\sum_{j=1}^{K}\pi_j\mathcal{N}(x_n|\mu_j,\Sigma_j)}\cdot \Sigma_k^{-1}(x_n-\mu_k)\\ =\sum_{n=1}^{N}\gamma(z_{nk})\Sigma_k^{-1}(x_n-\mu_k)=0 \\ \therefore \mu_k=\frac{\sum_{n=1}^{N}\gamma(z_{nk})x_n}{\sum_{n=1}^{N}\gamma(z_{nk})} \]</span> <span class="math inline">\(\mu\)</span> looks like a weighted mean of <span class="math inline">\(x\)</span>, but it is not a closed form solution.</p><p>Maximize <span class="math inline">\(\mathcal L\)</span> with respect to <span class="math inline">\(\Sigma\)</span> is difficult, so we optimize it with respect to <span class="math inline">\(\Sigma^{-1}\)</span>, which will be denoted as <span class="math inline">\(S\)</span> <span class="math display">\[ \nabla_{S}\mathcal{L}=\sum_{n=1}^{N}\frac{\pi_k}{\sum_{i=1}^{K}\pi_i\mathcal N(x_n|\mu_i, \Sigma_i)}\nabla_SN(x_n|\mu_k, \Sigma_k) \]</span></p><p><span class="math display">\[ \frac{\partial N(x_n|\mu_k, \Sigma_k)}{\partial S}=\frac{\partial N(x_n|\mu_k, \Sigma_k)}{\partial \log N(x_n|\mu_k, \Sigma_k)}\cdot\frac{\partial \log N(x_n|\mu_k, \Sigma_k)}{\partial S}\\ \because \frac{\partial N(x_n|\mu_k, \Sigma_k)}{\partial \log N(x_n|\mu_k, \Sigma_k)}=N(x_n|\mu_k, \Sigma_k) \\ \frac{\partial \log N(x_n|\mu_k, \Sigma_k)}{\partial S}=\frac{1}{2}\frac{\partial \log|S|}{\partial S}-\frac{1}{2}\cdot\frac{\partial (x_n-\mu_k)^\top S (x_n-\mu_k)}{\partial S}\\ =\frac{S^*}{2|S|}-\frac{1}{2}(x_n-\mu_k)(x_n-\mu_k)^\top \\ =\frac{1}{2}(\Sigma_k-(x_n-\mu_k)(x_n-\mu_k)^\top)\\ \therefore \frac{\partial N(x_n|\mu_k, \Sigma_k)}{\partial S}=\frac{1}{2}N(x_n|\mu_k, \Sigma_k)(\Sigma_k-(x_n-\mu_k)(x_n-\mu_k)^\top) \]</span></p><p><span class="math display">\[ \nabla_{S}\mathcal{L}=\sum_{n=1}^{N}\gamma(z_{nk})(\Sigma_k-(x_n-\mu_k)(x_n-\mu_k)^\top)=0 \]</span></p><p><span class="math display">\[ \Sigma_k=\frac{\sum_{n=1}^{N}\gamma(z_{nk})(x_n-\mu_k)(x_n-\mu_k)^\top}{\sum_{n=1}^{N}\gamma(z_{nk})} \]</span></p><p><span class="math display">\[ \because \nabla_{\pi_k}\mathcal{L}=\sum_{n=1}^{N}\frac{N(x_n|\mu_k, \Sigma_k)}{\sum_{i=1}^{K}\pi_i\mathcal N(x_n|\mu_i, \Sigma_i)}+\lambda=0 \\ \therefore \lambda\sum_{i=1}^{K}(\pi_k)+\sum_{n=1}^{N}\gamma(z_{nk})=0 \\ \therefore \lambda = -N = - \sum_{n=1}^{N}\sum_{k=1}^{K}\gamma(z_{nk}) \\ \therefore \pi_k=\frac{N_k}{N}=\frac{\sum_{n=1}^{N}\gamma(z_{nk})}{\sum_{n=1}^{N}\sum_{i=1}^{K}\gamma(z_{ni})} \]</span></p><h4 id="em-algorithm">EM algorithm</h4><p>Expection:</p><p>calulate <span class="math inline">\(\gamma(z_{nk})\)</span></p><p>Maximization:</p><p>update <span class="math inline">\(\mu, \Sigma, \pi\)</span></p><h4 id="variational-inference">Variational Inference</h4><p><span class="math display">\[ \log P(D)=\sum_{n=1}^{N}\log P(x_n)\\ =\sum_{n=1}^{N}\log (\sum_{z_n}q(z_n)\frac{p(z_n)p(x_n|z_n))}{q(z_n)}\\ =\sum_{n=1}^{N}\log \mathbb{E}_q[\frac{p(z_n)p(x_n|z_n))}{q(z_n)}]\\ \ge\sum_{n=1}^{N}\mathbb{E}_q[\log\frac{p(z_n)p(x_n|z_n))}{q(z_n)}]\\ =\sum_{n=1}^{N}\mathbb{E}_q[\log p(x_n, z_n)-\log q(z_n)] \]</span></p><p>Let <span class="math inline">\(\mathcal{L}_1=\sum_{n=1}^{N}\mathbb{E}_q[\log p(x_n, z_n)-\log q(z_n)]\)</span></p><p>The gap between <span class="math inline">\(\log P(D)\)</span> and <span class="math inline">\(\mathcal L_1\)</span>: <span class="math display">\[ \mathcal{L_1}=\sum_{n=1}^{N}\mathbb{E}_q[\log p(x_n, z_n)-\log q(z_n)]\\ =\sum_{n=1}^N\mathbb{E}_q[\log p(x_n)+\log p(z_n|x_n)-\log q(z_n)]\\ =\log p(D)-KL[q(Z)||p(Z|D)] \]</span></p><figure><img src="/StatisticalMachineLearning/clustering/image-20190419173117190.png" alt="image-20190419173117190"><figcaption>image-20190419173117190</figcaption></figure><p>Minimize the gap with respect to <span class="math inline">\(p(z)\)</span>: E <span class="math display">\[ \because \mathcal{L}=\log p(D)-KL[p(Z)||p(Z|D)]\\ \therefore q=\text{argmax}_{q} \mathcal{L}=\text{argmin}_q KL[q(Z)||p(Z|D)]\\ \therefore q(z_n)=p(z_n|x_n) \]</span> Maximize the lower bbound with respect to <span class="math inline">\(\Theta\)</span>: M</p><h3 id="em-example-multinomial-distribution">EM example: multinomial distribution</h3><figure><img src="/StatisticalMachineLearning/clustering/image-20190421141432302.png" alt="image-20190421141432302"><figcaption>image-20190421141432302</figcaption></figure><p>In the expectation step, we calculate the posterior <span class="math display">\[ \gamma_{dk}=P(c_d=k|d)\\ =\frac{p(d|c_d=k)p(k)}{p(d)}\\ =\frac {\pi_k\frac{n_d!}{\prod_wT_{dw}!}\prod_w\mu_{wk}^{T_{dw}}} {\sum_{k&#39;} \frac{n_d!}{\prod_wT_{dw}!}\prod_w\mu_{wk&#39;}^{T_{dw}}} \]</span> In the maximization step, apply MLE with <span class="math inline">\(\gamma_{dk}\)</span> fixed. <span class="math display">\[ \mathcal{L}_0=\log P(D)\\ =\sum_{d}\log P(d)\\ =\sum_{d}\log(\sum_k P(d|c_d=k)P(c_d=k)) \]</span></p><p>The lagrange function is that <span class="math display">\[ \mathcal{L}=\sum_{d}\log(\sum_k P(d|c_d=k)P(c_d=k))\\ -\sum_k[\lambda_k(\sum_w\mu_{wk}-1)]\\ -\beta(\sum_k \pi_k-1) \]</span></p><p><span class="math display">\[ \frac{d \mathcal{L}}{du_{wk}}=\sum_d \frac{p(d|c_d=k)p(c_d=k)\cdot\frac{T_{dw}}{\mu_{wk}}} {\sum_kp(d|c_d=k)p(c_d=k)}-\lambda_k=0\\ \therefore \sum_w(\sum_d\gamma_{dk}T_{dw}-\mu_{wk}\lambda_k)\\ \therefore \sum_d\gamma_{dk}T_d-\lambda_k=0\\ \therefore \mu_{wk}=\frac{\sum_d\gamma_{dk}T_{dw}}{\sum_d\gamma_{dk}T_d} \]</span></p><p><span class="math display">\[ \frac{d\mathcal{L}}{d\pi_k}=\sum_d\frac{P(d|c_d=k)}{\sum_kP(d|c_d=k)P(d_d=k)}-\beta=0\\ \because \sum_k(\sum_d\frac{P(d|c_d=k)\pi_k}{\sum_kP(d|c_d=k)P(d_d=k)}-\pi_k\beta)=0\\ \therefore \sum_k\sum_d\gamma_{dk}-\beta=0\\ \therefore \beta=D\\ \therefore \sum_d\gamma_{dl}-D\pi_k=0\\ \therefore \pi_k=\frac{\sum_d\gamma_{dk}}{D} \]</span></p></div><footer class="article-footer"></footer></div></article><article id="post-Proxmox Virtual Environment/unable_to_access_host" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Proxmox-Virtual-Environment/">Proxmox Virtual Environment</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Proxmox Virtual Environment/unable_to_access_host/"><time datetime="2020-03-27T09:10:54.080Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/Proxmox Virtual Environment/unable_to_access_host.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/Proxmox Virtual Environment/unable_to_access_host.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/Proxmox Virtual Environment/unable_to_access_host.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/Proxmox Virtual Environment/unable_to_access_host/">Unable to Access Host via PVE API</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="issue">Issue</h2><p>Host is alive and accessible, but all vm states are not accessable, and web GUI is not accessable either.</p><p>I can ssh to this host.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lsof -i :8006</span><br><span class="line"><span class="comment"># there are processes listening on it</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -s -k https://localhost:8006</span><br><span class="line"><span class="comment"># nothing respond</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tail /var/<span class="built_in">log</span>/pveproxy/access.log</span><br><span class="line"><span class="comment"># nothing</span></span><br></pre></td></tr></table></figure><h2 id="solution">Solution</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service pve-cluster restart</span><br></pre></td></tr></table></figure></div><footer class="article-footer"></footer></div></article><article id="post-Python/Einstein-Summation-Convention" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Python/">Python</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Python/Einstein-Summation-Convention/"><time datetime="2020-03-27T09:10:54.080Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/Python/Einstein-Summation-Convention.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/Python/Einstein-Summation-Convention.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/Python/Einstein-Summation-Convention.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/Python/Einstein-Summation-Convention/">Einstein Summation Convention</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="common-operations-in-this-notation">Common operations in this notation</h2><p><span class="math display">\[ \mathbf{u}\cdot\mathbf{v}=u_iv^i \]</span></p><p><span class="math display">\[ \mathbf{C}=\mathbf{A}\mathbf{B}\\ \Rightarrow C^i_k=A^i_{j}B^{j}_k \]</span></p><p><span class="math display">\[ trace(\mathbf{A})=A^i_{i} \]</span></p><h2 id="numpy-convention">NumPy Convention</h2><p><span class="math inline">\(trace(A)\)</span>: <code>ii</code></p><p><span class="math inline">\(diag(A)\)</span>: <code>ii-&gt;i</code></p><p><code>sum(A, axis=1)</code>: <code>ij-&gt;i</code></p><p><span class="math inline">\(A^\top\)</span>: <code>ij-&gt;ji</code></p><p><span class="math inline">\(\mathbf{u}\cdot\mathbf{v}\)</span>: <code>i,i</code></p><p><span class="math inline">\(\mathbf{A}\mathbf{u}\)</span>: <code>ij,j-&gt;i</code></p></div><footer class="article-footer"></footer></div></article><article id="post-StatisticalMachineLearning/boost" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/StatisticalMachineLearning/">StatisticalMachineLearning</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/StatisticalMachineLearning/boost/"><time datetime="2020-03-27T09:10:54.080Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/StatisticalMachineLearning/boost.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/StatisticalMachineLearning/boost.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/StatisticalMachineLearning/boost.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/StatisticalMachineLearning/boost/">boost</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="model-averaging-from-ct-to-boosting">Model Averaging: from CT to Boosting</h2><p>In generally, Boosting &gt; Random Forest &gt; Bagging &gt; Single Tree</p><h3 id="classification-trees">Classification Trees</h3><p>The decision boundaries are along the axes.</p><h3 id="bagging">Bagging</h3><p>Suppose <span class="math inline">\(C(S, x)\)</span> is a classifier for dataset <span class="math inline">\(S\)</span> and input point <span class="math inline">\(x\)</span>.</p><p>To bag <span class="math inline">\(C\)</span>, draw bootstrap samples <span class="math inline">\(S_1, S_2, ..., S_B\)</span> from <span class="math inline">\(S\)</span> with size <span class="math inline">\(N\)</span></p><p><span class="math inline">\(C_{\text{bag}}(x)=\text{Majority Vote}(C(S_i, x), i \in [1, B])\)</span></p><h3 id="random-forest">Random Forest</h3><p>refine bagging</p><p>At each tree split ,randomly sample <span class="math inline">\(m\)</span> features and only consider these samples. Typically <span class="math inline">\(m=\sqrt p\)</span> or <span class="math inline">\(m=\log p\)</span>, where <span class="math inline">\(p\)</span> is the total number of features.</p><p>RF tries to improve bagging by de-corelating the trees.s</p><h3 id="boosting">Boosting</h3><p>Average many trees, but each grown from reweighed samples. <span class="math display">\[ C(x)=\text{sign}\sum_{n}\alpha_nC_n(x) \]</span></p><h4 id="adaboost">Adaboost</h4><ol type="1"><li><p>initialize observations' weights <span class="math inline">\(w_i=\frac{1}{N}, i = 1, 2, ..., N\)</span></p></li><li><p>For m from 1 to M:</p><ol type="1"><li><p>fit <span class="math inline">\(C_m\)</span> with observations with weights <span class="math inline">\(w_i\)</span></p></li><li><p>Compute weighted error rate: <span class="math display">\[ err_m=\frac{\sum_i w_i1_{C_m(x_i)\neq y_i}}{\sum_i w_i} \]</span></p></li><li><p>Update ratio <span class="math inline">\(\alpha_m=\log\frac{1-err_m}{err_m}\)</span></p></li><li><p>Update weights <span class="math display">\[ w_i \leftarrow w_i \cdot \exp(\alpha_m1_{C_m(x_i)\neq y_i}) \]</span></p></li><li><p>Renormalize sum of <span class="math inline">\(w_i\)</span> to 1s</p></li></ol></li><li><p>return <span class="math inline">\(C(x)=\text{sign}(\sum_{i=1}^{M}\alpha_mC_m(x))\)</span></p></li></ol><p><a href="train_error_theorem.pdf">train_error_theorem</a></p><h4 id="additive-model">Additive Model</h4><p>Boosting build a additive model: <span class="math display">\[ f(x)=\sum_{k=1}^{M}\beta_kC(x;\gamma_k) \]</span> Traditional methods fit the parameters jointly. But Adaboost do it stagewisely.</p><h4 id="adaboost-stagewise-modeling">Adaboost: Stagewise Modeling</h4><p>Adaboost fit a stagewise logistic regression model <span class="math inline">\(f(x)\)</span> by stagewisely fit the loss: <span class="math display">\[ \mathcal{L}=\exp(-yf(x)) \]</span></p><p>Given <span class="math inline">\(f_{M-1}(x)\)</span>, the solution to <span class="math inline">\(\beta_M, \gamma_M\)</span> is <span class="math display">\[ \text{argmin}_{\beta, \gamma}\sum_{i=1}^{N}\exp(-y_i(f_{M-1}(x_i)+\beta C(x_i;\gamma))) \]</span></p><h5 id="why-exponential-loss">Why Exponential Loss</h5><figure><img src="/StatisticalMachineLearning/boost/image-20190419140355191.png" alt="image-20190419140355191"><figcaption>image-20190419140355191</figcaption></figure><p>exp loss is a upper bound of 0-1 loss.</p><p>It leads to simple reweighting scheme.</p><p>binomial deviance can be more robust</p><h4 id="general-stagewise-algorithm">General Stagewise Algorithm</h4><figure><img src="/StatisticalMachineLearning/boost/image-20190419140524731.png" alt="image-20190419140524731"><figcaption>image-20190419140524731</figcaption></figure><h2 id="learning-from-crowds">Learning from Crowds</h2></div><footer class="article-footer"></footer></div></article><article id="post-PaperNotes/gan_anomaly_detection" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/PaperNotes/">PaperNotes</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/PaperNotes/gan_anomaly_detection/"><time datetime="2020-03-27T09:10:54.076Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/PaperNotes/gan_anomaly_detection.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/PaperNotes/gan_anomaly_detection.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/PaperNotes/gan_anomaly_detection.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/PaperNotes/gan_anomaly_detection/">GAN Anomaly Detection</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="ganomaly-semi-supervised-anomaly-detection-via-adversarial-training">GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training</h2><p>Ref: Akcay, Samet, Amir Atapour-Abarghouei, and Toby P. Breckon. "GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training." <em>arXiv preprint arXiv:1805.06725</em>(2018).</p><h2 id="real-time-anomaly-detection-and-localization-in-crowded-scenes">Real-Time Anomaly Detection and Localization in Crowded Scenes</h2><p>Ref: Sabokrou, Mohammad, et al. "Real-time anomaly detection and localization in crowded scenes." <em>Proceedings of the IEEE conference on computer vision and pattern recognition workshops</em>. 2015.</p></div><footer class="article-footer"></footer></div></article><nav id="page-nav"><a class="extend prev" rel="prev" href="/index.md/">&laquo; Prev</a><a class="page-number" href="/index.md/">1</a><span class="page-number current">2</span><a class="page-number" href="/index.md/page/3/">3</a><a class="page-number" href="/index.md/page/4/">4</a><a class="extend next" rel="next" href="/index.md/page/3/">Next &raquo;</a></nav></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner">Zeyan LI &copy; 2020 <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png"></a><br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a></div></div></footer><script src="/libs/lightgallery/js/lightgallery.min.js"></script><script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script><script src="/libs/lightgallery/js/lg-pager.min.js"></script><script src="/libs/lightgallery/js/lg-autoplay.min.js"></script><script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script><script src="/libs/lightgallery/js/lg-zoom.min.js"></script><script src="/libs/lightgallery/js/lg-hash.min.js"></script><script src="/libs/lightgallery/js/lg-share.min.js"></script><script src="/libs/lightgallery/js/lg-video.min.js"></script><script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script async src="//cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/main.js"></script></div></body>