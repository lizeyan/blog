<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>Zeyan LI&#39;s Homepage</title><meta name="keywords" content="Zeyan LI&#39;s Homepage"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:type" content="website"><meta property="og:title" content="Zeyan LI&#39;s Homepage"><meta property="og:url" content="https://blog.lizeyan.me/index.md/page/4/index.html"><meta property="og:site_name" content="Zeyan LI&#39;s Homepage"><meta property="og:locale" content="en"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Zeyan LI&#39;s Homepage"><link rel="icon" href="/img/chitanta.png"><link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/libs/open-sans/styles.css"><link rel="stylesheet" href="/libs/source-code-pro/styles.css"><link rel="stylesheet" href="/css/style.css"><script src="/libs/jquery/2.1.3/jquery.min.js"></script><script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script><link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css"><link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css"><script type="text/javascript">!function(e,a,t,n,g,c,o){e.GoogleAnalyticsObject="ga",e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),o=a.getElementsByTagName(t)[0],c.async=1,c.src="//www.google-analytics.com/analytics.js",o.parentNode.insertBefore(c,o)}(window,document,"script"),ga("create","UA-138651056-1","auto"),ga("send","pageview")</script></head></html><body><div id="container"><header id="header"><div id="header-main" class="header-inner"><div class="outer"><a href="/" id="logo"><span class="site-title">Zeyan LI&#39;s Homepage</span></a><nav id="main-nav"><a class="main-nav-link" href="/">Home</a> <a class="main-nav-link" href="/archives">Archives</a> <a class="main-nav-link" href="/categories">Categories</a> <a class="main-nav-link" href="/tags">Tags</a> <a class="main-nav-link" href="/">About</a></nav><nav id="sub-nav"><div class="profile" id="profile-nav"><a id="profile-anchor" href="javascript:;"><img class="avatar" src="https://www.gravatar.com/avatar/07d6dce3cbd35d7c566b5f53f05dba2d"> <i class="fa fa-caret-down"></i></a></div></nav><div id="search-form-wrap"><form class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"> <button type="submit" class="search-form-submit"></button></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="Type something..."> <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"/",CONTENT_URL:"/content.json"}</script><script src="/js/insight.js"></script></div></div></div><div id="main-nav-mobile" class="header-sub header-inner"><table class="menu outer"><tr><td><a class="main-nav-link" href="/">Home</a></td><td><a class="main-nav-link" href="/archives">Archives</a></td><td><a class="main-nav-link" href="/categories">Categories</a></td><td><a class="main-nav-link" href="/tags">Tags</a></td><td><a class="main-nav-link" href="/">About</a></td><td><div class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"></div></td></tr></table></div></header><div class="outer"><aside id="profile"><div class="inner profile-inner"><div class="base-info profile-block"><img id="avatar" src="https://www.gravatar.com/avatar/07d6dce3cbd35d7c566b5f53f05dba2d?s=128"><h2 id="name">Zeyan LI</h2><h3 id="title">Ph.D. student</h3><span id="location"><i class="fa fa-map-marker"></i>Beijing, China</span> <a id="follow" target="_blank" href="https://github.com/lizeyan/">FOLLOW</a></div><div class="article-info profile-block"><div class="article-info-block">38 <span>posts</span></div><div class="article-info-block">3 <span>tags</span></div></div><div class="profile-block social-links"><table><tr><td><a href="https://github.com/lizeyan/" target="_blank" title="github" class="tooltip"><i class="fa fa-github"></i></a></td></tr></table></div></div></aside><aside id="sidebar"><div class="widget-wrap" id="categories"><h3 class="widget-title"><span>categories</span> &nbsp; <a id="allExpand" href="#"><i class="fa fa-angle-double-down fa-2x"></i></a></h3><ul class="unstyled" id="tree"><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Algorithm</a><ul class="unstyled" id="tree"><li class="file"><a href="/Algorithm/PC_algorithm/">PC Algorithm</a></li><li class="file"><a href="/Algorithm/association_rule_mining/">Association Rule Mining</a></li><li class="file"><a href="/Algorithm/locality_sensitive_hash/">Locality Sensitive Hashing</a></li><li class="file"><a href="/Algorithm/pagerank/">Page Rank</a></li><li class="file"><a href="/Algorithm/sequential_pattern_mining/">Sequential Pattern Mining</a></li><li class="file"><a href="/Algorithm/tsne/">t-SNE</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Conference</a><ul class="unstyled" id="tree"><li class="file"><a href="/Conference/ISSRE/"></a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; DeepLearningBook</a><ul class="unstyled" id="tree"><li class="file"><a href="/DeepLearningBook/optimization/">Optimization</a></li><li class="file"><a href="/DeepLearningBook/regularization/">Regularization</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Dreaming Dogs</a><ul class="unstyled" id="tree"><li class="file"><a href="/Dreaming Dogs/What is Dreaming Dogs/">Dreaming Dogs</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Latex</a><ul class="unstyled" id="tree"><li class="file"><a href="/Latex/cjk/">cjk</a></li><li class="file"><a href="/Latex/latex_snippets/">LaTex Snippets</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Linux</a><ul class="unstyled" id="tree"><li class="file"><a href="/Linux/bash_redirect_output/">Bash Redirect Output</a></li><li class="file"><a href="/Linux/ubuntu_1804_install_notes/">Ubuntu 18.04 install notes</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; PaperNotes</a><ul class="unstyled" id="tree"><li class="file"><a href="/PaperNotes/gan/">GAN</a></li><li class="file"><a href="/PaperNotes/gan_anomaly_detection/">GAN Anomaly Detection</a></li><li class="file"><a href="/PaperNotes/network_diffusion/">Network Diffusion</a></li><li class="file"><a href="/PaperNotes/train_gan/">Train GAN</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Proxmox Virtual Environment</a><ul class="unstyled" id="tree"><li class="file"><a href="/Proxmox Virtual Environment/unable_to_access_host/">Unable to Access Host via PVE API</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Python</a><ul class="unstyled" id="tree"><li class="file"><a href="/Python/Einstein-Summation-Convention/">Einstein Summation Convention</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; StatisticalMachineLearning</a><ul class="unstyled" id="tree"><li class="file"><a href="/StatisticalMachineLearning/boost/">boost</a></li><li class="file"><a href="/StatisticalMachineLearning/clustering/">Clustering</a></li><li class="file"><a href="/StatisticalMachineLearning/dimension_reduction/">Dimension Reduction</a></li><li class="file"><a href="/StatisticalMachineLearning/kernel_density_estimation/">Kernel Density Estimation</a></li><li class="file"><a href="/StatisticalMachineLearning/nb_and_lr/">NB and LR</a></li><li class="file"><a href="/StatisticalMachineLearning/reinforcement_learning/">Reinforcement Learning</a></li><li class="file"><a href="/StatisticalMachineLearning/svm/">SVM</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Statistics</a><ul class="unstyled" id="tree"><li class="file"><a href="/Statistics/test/">Statistical Test</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; configuration</a><ul class="unstyled" id="tree"><li class="file"><a href="/configuration/hexo_config/">Hexo Config</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; macOS</a><ul class="unstyled" id="tree"><li class="file"><a href="/macOS/install_homebrew/">Install Homebrew</a></li><li class="file"><a href="/macOS/install_supervisor/">Install supervisor via brew</a></li><li class="file"><a href="/macOS/macos_unable_to_fork_issue/">macOS Catalina unable to fork issue</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; nginx</a><ul class="unstyled" id="tree"><li class="file"><a href="/nginx/log_format/">log format of nginx</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; pypy</a><ul class="unstyled" id="tree"><li class="file"><a href="/pypy/install_pypy/">install PyPy</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; seafile</a><ul class="unstyled" id="tree"><li class="file"><a href="/seafile/client_conflicts/">Fix 'there is a conflict with an existing library'</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; 日本語</a><ul class="unstyled" id="tree"><li class="file"><a href="/日本語/キセキ/">キセキ</a></li><li class="file"><a href="/日本語/本文/">本文</a></li><li class="file"><a href="/日本語/用言/">用言</a></li></ul></li></ul></div><script>$(document).ready(function(){var r="fa-folder-open",i="fa-folder",l="fa-angle-double-down",d="fa-angle-double-up";$(document).on("click",'#categories a[data-role="directory"]',function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(r),l=$(this).siblings("ul");e.removeClass(r).removeClass(i),s?(void 0!==l&&l.slideUp({duration:100}),e.addClass(i)):(void 0!==l&&l.slideDown({duration:100}),e.addClass(r))}),$('#categories a[data-role="directory"]').bind("contextmenu",function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(r),l=$(this).siblings("ul"),d=$.merge(l.find("li ul"),l),o=$.merge(l.find(".fa"),e);o.removeClass(r).removeClass(i),s?(d.slideUp({duration:100}),o.addClass(i)):(d.slideDown({duration:100}),o.addClass(r))}),$(document).on("click","#allExpand",function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(l);e.removeClass(l).removeClass(d),s?($("#sidebar .fa.fa-folder").removeClass("fa-folder").addClass("fa-folder-open"),$("#categories li ul").slideDown({duration:100}),e.addClass(d)):($("#sidebar .fa.fa-folder-open").removeClass("fa-folder-open").addClass("fa-folder"),$("#categories li ul").slideUp({duration:100}),e.addClass(l))})})</script><div id="toTop" class="fa fa-angle-up"></div></aside><section id="main"><article id="post-Algorithm/tsne" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Algorithm/tsne/"><time datetime="2020-03-27T09:10:54.068Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/Algorithm/tsne.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/Algorithm/tsne.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/Algorithm/tsne.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/Algorithm/tsne/">t-SNE</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="intro">Intro</h2><h2 id="sne">SNE</h2><p><strong>Input</strong>: <span class="math display">\[ \{\mathbf{x}_i, \mathbf{x}_i\in \mathbb{R}^D\}_{i=1}^{N} \]</span> <strong>Output</strong>: <span class="math display">\[ \{\mathbf{y}_i, \mathbf{y}_i\in \mathbb{R}^d\}_{i=1}^{N}, d&lt;D \]</span> <strong>Objective</strong></p><p>SNE uses a conditional likelihood to measure the distance between two points, <span class="math inline">\(p_{i|j}\)</span>, which represents the probability to pick <span class="math inline">\(j\)</span> when <span class="math inline">\(i\)</span> is chosen. <span class="math display">\[ p_{j|i}=\frac{\exp(-\frac{||\mathbf{x}_i-\mathbf{x}_j||^2}{2\sigma_i^2})}{\sum_{k\neq i}\exp(-\frac{||\mathbf{x}_i-\mathbf{x}_k||^2}{2\sigma_i^2})} \]</span> Similarly, in the low dimension space, SNE uses <span class="math inline">\(q_{j|i}\)</span> to measure the distance between two mapping points. <span class="math display">\[ q_{j|i}=\frac{\exp(-||\mathbf{y}_i-\mathbf{y}_j||^2)}{\sum_{k\neq i}\exp(-||\mathbf{y}_i-\mathbf{y}_k||^2)} \]</span> The objective function is to minimize the KL divergence between the distance distribution of original space and mapping space. <span class="math display">\[ C=\text{KL}[P||Q]=\sum_i \sum_j p_{j|i}(\log{p_{j|i}}-\log{q_{j|i}}) \]</span> <strong>Estimation of <span class="math inline">\(\sigma_i\)</span></strong></p><p><strong>Optimization</strong></p><p>Gradient descent with momentum.</p><h2 id="t-sne">t-SNE</h2></div><footer class="article-footer"></footer></div></article><article id="post-Conference/ISSRE" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><div class="article-entry" itemprop="articleBody"></div><footer class="article-footer"></footer></div></article><article id="post-DeepLearningBook/optimization" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/DeepLearningBook/">DeepLearningBook</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/DeepLearningBook/optimization/"><time datetime="2020-03-27T09:10:54.068Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/DeepLearningBook/optimization.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/DeepLearningBook/optimization.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/DeepLearningBook/optimization.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/DeepLearningBook/optimization/">Optimization</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="深度学习中的学习和纯优化问题有何不同">8.1 深度学习中的学习和纯优化问题有何不同</h2><p>机器学习通常不能用直接的方式学习。我们的目标是某个性能指标<span class="math inline">\(P\)</span>，但是<span class="math inline">\(P\)</span>是和测试集有关的，而且可能本身就是intractable的。为此我们通常是定义一个损失函数<span class="math inline">\(J\)</span>，希望优化<span class="math inline">\(J\)</span>能够提升<span class="math inline">\(P\)</span>。</p><p>我们的目标是优化在真实数据分布下的期望损失:</p>$$ \newcommand{\vv}[1]{\boldsymbol{#1}} J^{*}(\vv \theta)=\mathbb{E}_{p_{data}}[\mathcal{L}(f(\vv{x};\vv{\theta}), \vv{y})] $$<h3 id="经验风险">经验风险</h3><p>在机器学习问题中，我们是不知道真实数据分布的，我们只有有限个数据样本，这些数据样本的经验分布为<span class="math inline">\(\hat p_{data}\)</span>。因此我们只能优化经验风险: <span class="math display">\[ \mathbb{E}_{\vv{x}\sim \hat p_{data}}[\mathcal{L}(f(\vv{x};\vv{\theta}), \vv{y})] \]</span></p><p>但是经验风险有两个问题：</p><ol type="1"><li>经验风险容易过拟合。容量足够大的模型往往可以直接记住整个训练集。</li><li>常用的高效的优化方法都是基于梯度的。但是准确的损失函数，比如0-1损失函数，往往无法提供梯度。</li></ol><h3 id="替代损失函数和early-stopping">替代损失函数和early stopping</h3><p>用来替代准确的损失函数，提供梯度信息的损失函数被称为surrogate loss。比如我们一般使用NLL替代0-1损失函数。替代损失函数可能会取得更好的性能，比如在0-1损失已经是0的时候，NLL还可以继续优化。因为即使我们找到了完美的分界面，我们还可以继续优化使得不同类数据分得尽量开。</p><p>另一方面的差别是机器学习中一般不会令学习在达到局部极小点才终止。也就是early stopping技术。在验证集上我们可以使用准确的0-1 loss，当验证集上损失不在下降就可以停止训练。在往后训练就会发生过拟合，但此时训练集上梯度往往还比较大。</p><h3 id="minibatch">minibatch</h3><p>估计梯度为以下的过程： <span class="math display">\[ \nabla J =\mathbb{E}_{\hat p_{data}}[\nabla \mathcal{L}(f(\vv{x};\vv{\theta}), \vv{y})] \]</span></p></div><footer class="article-footer"></footer></div></article><article id="post-DeepLearningBook/regularization" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/DeepLearningBook/">DeepLearningBook</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/DeepLearningBook/regularization/"><time datetime="2020-03-27T09:10:54.068Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/DeepLearningBook/regularization.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/DeepLearningBook/regularization.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/DeepLearningBook/regularization.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/DeepLearningBook/regularization/">Regularization</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="regularization">Regularization</h1><h2 id="paramter-norm-penalties">Paramter Norm Penalties</h2><h3 id="l2-norm">L2 Norm</h3><p>L2正则化也被称为ridge回归或者Tikhonov回归</p><p>假设参数只有权重<span class="math inline">\(\boldsymbol{w}\)</span>，正则化系数为<span class="math inline">\(\alpha\)</span>，则</p>$$ \newcommand{\vv}[1]{\boldsymbol{#1}} \hat{J}=J+\frac{1}{2}\alpha \vv{w}^\top\vv{w} \\ \nabla{\hat{J}}=\nabla{J}+\alpha \vv{w} $$ $$ \vv{w}\leftarrow \vv{w}-\epsilon (\alpha \vv{w}+\nabla J) \\ \Leftrightarrow \vv{w} \leftarrow (1-\epsilon \alpha) \vv{w} - \epsilon \nabla{J} $$<p>L2正则化可以看成MAP Bayesian Estimation。</p><p>假设损失函数<span class="math inline">\(J(\vv{\theta}|\vv{X},\vv{Y})\)</span>为最大似然估计MLE(<span class="math inline">\(-\log P_\text{model}(\vv{\theta}|\vv{X},\vv{Y})\)</span>)，则在MAP中，若假设参数的先验分布为 <span class="math display">\[ \vv{\theta}\sim\mathcal{N}(\vv{0}, \frac{1}{\alpha}\mathbf{I}) \]</span> 则MAP估计得到损失函数为 <span class="math display">\[ \hat{J}=-\log{P_\text{model}(\vv{\theta}|\vv{X},\vv{Y})}-\log{P(\vv{\theta})}\\ =J+\alpha \vv{\theta}^\top\vv{\theta} \]</span></p><hr><p>我们考虑一个简单的情况，损失函数是连续的二次函数，泰勒展开三阶及以上余项为0 <span class="math display">\[ \vv{w}^*=\text{argmin}_\vv{w} J(\vv{w})\\ \Rightarrow J(\vv{w})=J(\vv{w}^*)+\frac{1}{2}(\vv{w}-\vv{w}^*)^\top\mathbf{H}(\vv{w}-\vv{w}^*)\\ \Rightarrow \nabla \hat J=\mathbf{H}(\vv{w}-\vv{w}^*)+\alpha\vv{w} \]</span> 此时损失函数<span class="math inline">\(\hat J\)</span>的最优解应满足 <span class="math display">\[ \tilde{\vv{w}}=(\mathbf{H}+\alpha\mathbf{I})^{-1}\mathbf{H}\vv{w}^* \]</span> 将<span class="math inline">\(\mathbf{H}\)</span>做正交对角化，得到 <span class="math display">\[ \tilde{\vv{w}}=\mathbf{Q}(\mathbf{\Lambda}+\alpha\mathbf{I})^{-1}\mathbf{\Lambda}\mathbf{Q}^\top\vv{w}^* \]</span> 考虑上式系数矩阵的特征值,<span class="math inline">\(\tilde{\lambda_i}=\frac{\lambda_i}{\lambda_i+\alpha}\)</span>，我们发现特征值比较大的方向上，基本没有变化；特征值比较小的方向，则权重会减小。</p><p>考虑最简单的情况，线性拟合，使用均方误差，则<span class="math inline">\(\vv{w}\)</span>有最优解： <span class="math display">\[ J=(\mathbf{X}\vv{w}-\vv{y})^\top(\mathbf{X}\vv{w}-\vv{y}) \\ \hat J = (\mathbf{X}\vv{w}-\vv{y})^\top(\mathbf{X}\vv{w}-\vv{y})+\frac{1}{2}\alpha\vv{w}^\top\vv{w} \]</span></p><p>此时的最优解分别为 <span class="math display">\[ \boldsymbol w=(\mathbf{X}\mathbf{X}^\top)^{-1}\mathbf{X}^\top\boldsymbol w \\ \hat{\boldsymbol w}=(\mathbf{X}\mathbf{X}^\top + \alpha \mathbf{I})^{-1}\mathbf{X}^\top\boldsymbol w \]</span></p><h3 id="l1-norm">L1 Norm</h3><h2 id="norm-penalties-as-constrained-optimazation">Norm Penalties as constrained optimazation</h2><p>将有范数惩罚项的优化目标看作有约束的最优化问题。</p><h2 id="regularization-and-under-constrained-problems">Regularization and under-constrained problems</h2><h2 id="dataset-augmentation">Dataset augmentation</h2><p>通过数据增强增加泛化性能。</p><p>对于神经网络，数据增强有着特别重要的意义。数据增强的一种方式就是对输入数据加入随机噪声。Dropout可以看作是一种数据增强方法。</p><h2 id="noise-robustness">Noise Robustness</h2><p>一般加入噪声比只是约束参数的大小效果更好，尤其是在隐藏层加入噪声。</p><p>另一种正则化模型的方法是给权重增加噪声。这对应的是模型权重的不确定性。</p><p>给权重增加噪声可以使模型找到对权重变化不敏感的极小点。</p><p>一些数据集label有错误。为了避免这种危害，可以直接对label的不确定性建模。</p><p>比如在多分类问题中，softmax输出层的目标是0-1。可以设定一个label错误的概率<span class="math inline">\(\epsilon\)</span>，将目标改为<span class="math inline">\((\frac{\epsilon}{k})\)</span>-<span class="math inline">\((1-\frac{\epsilon}{k-1})\)</span></p><p>当目标是0-1的时候，softmax永远不可能学习到0-1，模型永远不会收敛，此时参数只是无限制地增大。我们也可以用其他正则化的方式，比如weight decay解决这个问题。</p><h2 id="semi-supervised-learning">Semi-Supervised Learning</h2><h2 id="multi-task-learning">Multi-Task Learning</h2><h2 id="early-stopping">Early Stopping</h2><figure><img src="/DeepLearningBook/regularization/image-20190113144830928.png" alt="image-20190113144830928"><figcaption>image-20190113144830928</figcaption></figure><p>Early stopping可以看作是一种选择超参"训练时间"的方法</p><p>使用验证集会导致有一部分训练集无法被模型使用。这时可以重新使用整个数据集进行训练。</p><p>Early stopping的正则化作用机制是限制了参数的变化范围</p><figure><img src="/DeepLearningBook/regularization/image-20190113154117157.png" alt="image-20190113154117157"><figcaption>image-20190113154117157</figcaption></figure><p>以线性回归为例，在适当的条件下early stopping和L2 norm regularization是等价的。</p><p>但是early stopping相比weight decay有着不需要调整超参，只需要根据验证集上的误差就足够了。</p><h2 id="parameter-tryingparameter-sharing">Parameter Trying，Parameter Sharing</h2><p>有时我们会希望限制模型中的参数互相之间比较接近。</p><p>一种方式是加入显式的约束。<span class="math inline">\(\alpha ||\boldsymbol w_a - \boldsymbol w_b||_2\)</span>。</p><p>实际上可以直接让这样的参数相等，这种方法就是parameter sharing。parameter sharing可以只存储一份参数，减少了内存开销。</p><p>CNN就是parameter sharing的例子。</p><h2 id="sparse-represetations">Sparse Represetations</h2><p>参数的L1正则化会形成稀疏的模型：</p><figure><img src="/DeepLearningBook/regularization/image-20190113160610461.png" alt="image-20190113160610461"><figcaption>image-20190113160610461</figcaption></figure><p>而这里我们希望得到稀疏的表示：</p><figure><img src="/DeepLearningBook/regularization/image-20190113160630989.png" alt="image-20190113160630989"><figcaption>image-20190113160630989</figcaption></figure><p>可以仿照参数的L1正则化： <span class="math display">\[ \tilde J=J+\alpha \Omega(\boldsymbol h) \]</span> 或者使用orthogonal matching pursuit （OMP）显式地限制<span class="math inline">\(\boldsymbol h\)</span>为稀疏的： <span class="math display">\[ \text{argmin}_{\boldsymbol h, ||\boldsymbol h||_0 &lt; k} ||\boldsymbol x - \mathbf{W} \boldsymbol h||_2 \]</span></p><h2 id="bagging-and-other-ensemble-method">Bagging and Other Ensemble Method</h2><blockquote><p>Bagging (short for bootstrap aggregating) is a technique for reducing generalization error by combining several models</p></blockquote><blockquote><p>on average, the ensemble</p><p>will perform at least as well as any of its members, and if the members make</p><p>independent errors, the ensemble will perform signiﬁcantly better than its members</p></blockquote><h2 id="dropout">Dropout</h2><p>Dropout的两种approximate方法：metro carlo，直接使用全部单元。后者计算开销几乎没有，但是性能两者在不同的问题互有上下。</p><blockquote><p>One of the key insights of dropout is that training a network with stochastic</p><p>behavior and making predictions by averaging over multiple stochastic decisions</p><p>implements a form of bagging with parameter sharing</p></blockquote><p>对模型的随机改变并不局限于随机丢弃某些单元。例如将权重乘以一个随机变量也是有效的。</p><p>Dropout的另一个作用：</p><blockquote><p>This means each hidden unit must be able to perform well regardless of which other hidden units are in the model</p><p>Dropout thus regularizes each hidden unit to be not merely a good feature but a feature that is good in many contexts</p></blockquote><h2 id="advertisial-training">Advertisial Training</h2><h2 id="tangent-distance-tangent-prop-and-manifold-tangent-classiﬁer">Tangent Distance, Tangent Prop, and Manifold Tangent Classiﬁer</h2></div><footer class="article-footer"></footer></div></article><article id="post-Algorithm/PC_algorithm" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Algorithm/PC_algorithm/"><time datetime="2020-03-27T09:10:54.064Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/Algorithm/PC_algorithm.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/Algorithm/PC_algorithm.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/Algorithm/PC_algorithm.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/Algorithm/PC_algorithm/">PC Algorithm</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="pc-algorithm">PC Algorithm</h1><h2 id="find-the-equivalence-class-of-a-dag">Find the Equivalence Class of a DAG</h2><h3 id="concepts-and-notations">Concepts and Notations</h3><h4 id="faithfulnessjmlr2005">Faithfulness<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></h4><p>A probability distribution <span class="math inline">\(P\)</span> is said to be faithful with respect to a graph <span class="math inline">\(G\)</span>, if conditional independencies of the distribution can be inferred from <strong>d-seperation</strong> in the graph <span class="math inline">\(G\)</span> and vice-visa. More precisely, consider a random vector <span class="math inline">\(\mathbf{X}\sim P\)</span>. Faithfulness of <span class="math inline">\(P\)</span> with respect to <span class="math inline">\(G\)</span> means, for any <span class="math inline">\(i\ne j\in V\)</span> and any set <span class="math inline">\(\mathbf{s}\in V\)</span>,</p><p><span class="math display">\[ \mathbf{X}^{(i)}\bot \mathbf{X}^{(j)}\ \text{given} \ \mathbf{s} \\\leftrightarrow\\ i\ \text{and}\ j \text{are d-seperated by the set}\ s \]</span></p><h4 id="d-seperationmit6.034">D-Seperation<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></h4><p>The Bayes net assumption says, "each variable is conditionally independent of its non-descendants, given its parents".</p><p>D-seperation is a formal procedure using this statement.</p><ol type="1"><li><p>Draw the ancestral graph.</p></li><li><p>For each pair of variables with a common child, draw a undirected edge between them.</p></li><li><p>Replace directed edges with undirected edges.</p></li><li><p>Delete the givens and their edges.</p></li><li><p>If the variables are disconnected, or one or more are missing, they are guaranteed to be independent. 'Connected' means there is a path between them, even though they are not directly connected:</p><p>Example:</p></li></ol><figure><img src="/Algorithm/PC_algorithm/image-20190715161555478.png" alt="image-20190715161555478"><figcaption>image-20190715161555478</figcaption></figure><p>A and B are not independent given D and F.</p><p>A and B are marginally independent.</p><p>A and B are not independent given C.</p><p>D and E are independent given C.</p><p>D and E are not marginally independent.</p><p>D and E are not independent given A and B.</p><h4 id="skeleton-and-v-struture">Skeleton and V-Struture</h4><p>The skeleton of a DAG <span class="math inline">\(G\)</span> is the undirected graph obtained from <span class="math inline">\(G\)</span> by substudting undirected edges for directed edges.</p><p>A v-structure is an ordered triple <span class="math inline">\((i, j, k)\)</span> such that <span class="math inline">\(G\)</span> contains <span class="math inline">\(i\to j\)</span> and <span class="math inline">\(j\to k\)</span> but no <span class="math inline">\(i\to k\)</span>.</p><h4 id="equivalent">Equivalent</h4><p>Two DAG are equilavent if and only if they have the same skeleton and the san v-structures.</p><h3 id="find-the-skeleton-using-pc-algorithmjmlr2005">Find The Skeleton Using PC Algorithm<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></h3><figure><img src="/Algorithm/PC_algorithm/image-20190715171557495.png" alt="image-20190715171557495"><figcaption>image-20190715171557495</figcaption></figure><p>Consider a DAG <span class="math inline">\(G\)</span> and assume that the distribution <span class="math inline">\(P\)</span> is faithful to <span class="math inline">\(G\)</span>. Denote the maximal number of neighbors by <span class="math inline">\(q=\max_{1\le j\le p}|adj(G, j)|\)</span>. Thenm the <span class="math inline">\(PC_{pop}\)</span> algorithm constructs the true skeleton of the DAG.</p><h3 id="sample-version-of-pc-algorithmjmlr2005">Sample Version of PC algorithm<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></h3><section class="footnotes" role="doc-endnotes"><hr><ol><li id="fn1" role="doc-endnote"><p>Kalisch, Markus, and Peter Bühlmann. "Estimating high-dimensional directed acyclic graphs with the PC-algorithm." <em>Journal of Machine Learning Research</em> 8.Mar (2007): 613-636.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li><li id="fn2" role="doc-endnote"><p>http://web.mit.edu/jmn/www/6.034/d-separation.pdf<a href="#fnref2" class="footnote-back" role="doc-backlink">↩</a></p></li><li id="fn3" role="doc-endnote"><p>Kalisch, Markus, and Peter Bühlmann. "Estimating high-dimensional directed acyclic graphs with the PC-algorithm." <em>Journal of Machine Learning Research</em> 8.Mar (2007): 613-636.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩</a></p></li><li id="fn4" role="doc-endnote"><p>Kalisch, Markus, and Peter Bühlmann. "Estimating high-dimensional directed acyclic graphs with the PC-algorithm." <em>Journal of Machine Learning Research</em> 8.Mar (2007): 613-636.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩</a></p></li></ol></section></div><footer class="article-footer"></footer></div></article><article id="post-Algorithm/association_rule_mining" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Algorithm/association_rule_mining/"><time datetime="2020-03-27T09:10:54.064Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/Algorithm/association_rule_mining.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/Algorithm/association_rule_mining.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/Algorithm/association_rule_mining.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/Algorithm/association_rule_mining/">Association Rule Mining</a></h1></header><div class="article-entry" itemprop="articleBody"><h1 id="association-rule-mining">Association Rule Mining</h1><p>Association rule mining can be viewed as a two-step process:</p><ol type="1"><li>Mining all frequent itemsets.</li><li>Generate strong association rules from the frequent itemsets.</li></ol><p>A itemset X is closed if there is no proper super-set Y such that Y has the same support count as X.</p><p>A itemset X is a closed frequent itemset if X is frequent and closed.</p><p>A itemset X is a maximal frequent if X is frequent and there is no proper super-set Y such that Y is frequent.</p><h2 id="frequent-itemset-mining-methods">Frequent Itemset Mining Methods</h2><h3 id="apriori">Apriori</h3><p>Generate candidates and test if they are frequent.</p><p>The key is pruning the search space.</p><p>The Apriori property: If a itemset is not frequent, then any superset of it is not frequent.</p><p>The procedure of Apriori algorithm:</p><ol type="1"><li>generate length-<span class="math inline">\(k\)</span> candidates based on length-<span class="math inline">\((k-1)\)</span> frequent itemsets.</li><li>Scan the database once and prune the infrequent length-<span class="math inline">\(k\)</span> candidates.</li></ol><h4 id="improve-the-efficiency-of-apriori">Improve the Efficiency of Apriori</h4><ol type="1"><li>Hash-based technique Hash the itemsets into buckets. If a candidate's corresponding bucket count is below the support threshold, then we need not to test it. It is especially useful when <span class="math inline">\(k=2\)</span>.</li><li>Transaction reduction A transaction does not contain any length-<span class="math inline">\(k\)</span> frequent itemset, cannot contain any length-<span class="math inline">\((k+1)\)</span> frequent itemset.</li><li>Partitioning Partition the search space into some subspaces. Any global frequent itemset must be local frequent itemset in at least one subspace (with support threshold in subspace changed)</li><li>Sampling.</li><li>Dynamic itemset counting</li></ol><h3 id="frequent-pattern-grouwth">Frequent-Pattern Grouwth</h3></div><footer class="article-footer"></footer></div></article><article id="post-Algorithm/locality_sensitive_hash" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Algorithm/locality_sensitive_hash/"><time datetime="2020-03-27T09:10:54.064Z" itemprop="datePublished">2020-03-27</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/Algorithm/locality_sensitive_hash.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/Algorithm/locality_sensitive_hash.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/Algorithm/locality_sensitive_hash.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/Algorithm/locality_sensitive_hash/">Locality Sensitive Hashing</a></h1></header><div class="article-entry" itemprop="articleBody"><p>[TOC]</p><h2 id="definition-and-instances">Definition and Instances</h2><p>LSH is a randomized hasing framework for efficient approximate neasrest negibot search in high dimensional space.</p><p>It is based on the definition of LSH family <span class="math inline">\(\mathcal{H}\)</span>, a family of hash functions mapping similar input items into the same hash code with higher probability than dissimilar items.</p><p>LSH aims to maximize the probability of collision of similar items, while tradition hash always avoid collisions.</p><h3 id="the-family">The Family</h3><p>A family <span class="math inline">\(\mathcal{H}\)</span> is <span class="math inline">\((R, cR, P_1, P_2)\)</span>-sensitive if for any two items <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span></p><p>If <span class="math inline">\(d(p, q)\le R\)</span>, then <span class="math inline">\(P(h(p)=h(q))\ge P_1​\)</span></p><p>If <span class="math inline">\(d(p, q)\ge cR\)</span>, then <span class="math inline">\(P(h(p)=h(q))\le P_2\)</span></p><p>Here <span class="math inline">\(c&gt;1\)</span>, <span class="math inline">\(P_1&gt;P_2\)</span>, <span class="math inline">\(h\in\mathcal{H}\)</span></p><p>Define <span class="math inline">\(\rho=\frac{\log P_1}{\log P_2}\)</span>, then there exists an algorithm for (R, C)-near neighbor problem which uses <span class="math inline">\(O(dn+n^{1+\rho})\)</span> space, with query time dominated by <span class="math inline">\(O(n^{\rho})\)</span> distance computations and <span class="math inline">\(O(n^{\rho}\log_{1/P_2}n)\)</span> evaluations of hash functions. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><p>Define <span class="math inline">\(g(x)=(h_1(x), ...., h_K(x))\)</span>, the output of <span class="math inline">\(g\)</span> identifies a hash bucket id.</p><p>However, the compound hash function also reduce the probability of collsion of smiliar items.</p><p>To improve the recall, <span class="math inline">\(L\)</span> such compund hash function are sampled independently, each of which corresponds to a hash table.</p><p>To improve precision, K should be large.</p><p>To improve recall, L should be large.</p><p>The items lying in the L hash buckets are retrieved as near item candidates.</p><h3 id="l_p-distance"><span class="math inline">\(l_p\)</span> Distance</h3><h3 id="angle-based-distance">Angle-Based Distance</h3><h3 id="hamming-distance">Hamming Distance</h3><h3 id="jaccard-coefficient">Jaccard Coefficient</h3><h3 id="mathcalx2-distance"><span class="math inline">\(\mathcal{X}^2\)</span> Distance</h3><h3 id="rank-similarity">Rank Similarity</h3><h3 id="shift-invariant-kernels">Shift Invariant Kernels</h3><h3 id="non-metric-distance">Non-Metric Distance</h3><h3 id="arbitrary-distance-measures">Arbitrary Distance Measures</h3><h2 id="search-modeling-and-analyzing">Search, Modeling and Analyzing</h2><h3 id="search">Search</h3><h4 id="entropy-based-search">Entropy-based search</h4><h4 id="lsh-forest">LSH forest</h4><h4 id="adaptative-lsh">Adaptative LSH</h4><h4 id="multi-probe-lsh-multi-probe-lsh">Multi-Probe LSH <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></h4><p>Given a query <span class="math inline">\(q\)</span>, the basic LSH query <span class="math inline">\(g(q)=(h_1(q), ..., h_M(q))\)</span>, while multi-probe LSH probes <span class="math inline">\(g(q)+\Delta\)</span>. <span class="math inline">\(\Delta=(\delta_1, ..., \delta_M), \delta_i\in\{-1, 0, 1\}\)</span>, since similar objects should hash to the same or adjacent buckets with high probability. A approriate pertubation sequence will make multi-probe LSH achieves similar recall with less hash tables and similar time complexity.</p><h5 id="step-wise-probing-sequence">Step-Wise Probing Sequence</h5><p>Firstly probe the 1-step buckets, then all the 2-step buckets, and so on.</p><p>The total number of all <span class="math inline">\(n​\)</span>-step buckets is <span class="math inline">\(L\times {M\choose n}\times2^n​\)</span>.</p><figure><img src="/Algorithm/locality_sensitive_hash/image-20190525212643536.png" alt="image-20190525212643536"><figcaption>image-20190525212643536</figcaption></figure><h5 id="query-directed-probing-sequence">Query-Directed Probing Sequence</h5><p>Using the step-wise probing method, all coordinates in the hash values are treated identically. And the probability of adding 1 and substracting 1 from each coordinate is equal as well.</p><figure><img src="/Algorithm/locality_sensitive_hash/image-20190525213133478.png" alt="image-20190525213133478"><figcaption>image-20190525213133478</figcaption></figure><p>Consider the hash function <span class="math inline">\(h(q)=\lfloor\frac{a\cdot q+b}{W}\rfloor​\)</span>.</p><p>Let <span class="math inline">\(x_i(\delta)​\)</span> ne the distance of <span class="math inline">\(q​\)</span> from the boundary of the slot <span class="math inline">\(h_i(q)+\delta​\)</span>, then <span class="math inline">\(x_i(-1)=f_i(q)-W\cdot f_i(q)​\)</span>, where <span class="math inline">\(f_i(q)=a\cdot q + b​\)</span>. <span class="math inline">\(x_i(1)=W-x_i(-1)​\)</span>. And we define <span class="math inline">\(x(0)=0​\)</span></p><p>For any fixed point <span class="math inline">\(p\)</span>, <span class="math inline">\(f_i(p)-f_i(q)\)</span> is a Gaussian random variable (<span class="math inline">\(a\)</span> is a sampled from a standard Gaussian), with 0 mean, and the variance is <span class="math inline">\(||p-q||_2^2\)</span>.</p><figure><img src="/Algorithm/locality_sensitive_hash/image-20190525215318390.png" alt="image-20190525215318390"><figcaption>image-20190525215318390</figcaption></figure><p>Then it indicates that <span class="math inline">\(score(\Delta)=\sum_{i=1}^{M}x_i(\delta_i)^2\)</span>. Pertutation vector with smaller score should have higher probability of yielding points near to <span class="math inline">\(q\)</span>.</p><p>Then we firstly calculate <span class="math inline">\(x_i(\delta), i=1, 2, ..., M, \delta\in\{-1,1\}\)</span>. We sort these <span class="math inline">\(2M\)</span> valus in increasing order. Let <span class="math inline">\(z_j\)</span> denote the <span class="math inline">\(j\)</span>th element in this sorted order.</p><p>Let <span class="math inline">\(\pi_j=(i, \delta)\)</span> if <span class="math inline">\(z_j=x_i(\delta)\)</span>. Since <span class="math inline">\(x_i(-1)+x_i(1)=W\)</span>, if <span class="math inline">\(\pi_j=(i, \delta)\)</span>, then <span class="math inline">\(\pi_{2M+1-j}=(i, -\delta)\)</span>.</p><p>Then the problem reduces to the problem of generating perturbation sets in increasing order of their scores.</p><figure><img src="/Algorithm/locality_sensitive_hash/image-20190526142038241.png" alt="image-20190526142038241"><figcaption>image-20190526142038241</figcaption></figure><figure><img src="/Algorithm/locality_sensitive_hash/image-20190526142045937.png" alt="image-20190526142045937"><figcaption>image-20190526142045937</figcaption></figure><h5 id="optimized-probing-sequence-construction">Optimized Probing Sequence Construction</h5><p>To avoid the overhead of maintaining the querying such a heap at query time, we precompute a certain sequence and reduce the generation of pertibation vectors to performing lookups instead of heap queries and updates.</p><p>We approximate the <span class="math inline">\(z_j^2\)</span> values by their expectations.</p><p>Note that <span class="math inline">\(x_i(\delta)\)</span> is uniformly distributed in <span class="math inline">\([0, W]\)</span>, and <span class="math inline">\(x_i(\delta)+x_i(-\delta)=W\)</span>.</p><p>The joint distribution of <span class="math inline">\(z_j\)</span> for <span class="math inline">\(j=1,2,...,M\)</span> is the following: pick <span class="math inline">\(M\)</span> numbers uniformaly and at random from <span class="math inline">\([0, \frac{W}{2}]\)</span>. <span class="math inline">\(z_j\)</span> is the <span class="math inline">\(j\)</span>-th largest nuber in this set. This is a well studied distribution. <span class="math display">\[ E[z_j]=\frac{j}{2(M+1)}W\\ E[z_j^2]=\frac{j(j+1)}{4(M+1)(M+2)}W^2 \]</span></p><h4 id="dynamic-collision-counting-for-search-c2lsh">Dynamic Collision Counting for Search <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></h4><p>Use dynamic compound hash function rather than a static one.</p><p>C2LSH firstly randomly chooses a set of <span class="math inline">\(m\)</span> LSDH functions with appropriately small interval <span class="math inline">\(W\)</span>, which form a function base <span class="math inline">\(\mathcal{B}​\)</span>.</p><p>Only data objects with large enough collison counts need to have their distances computed.</p><h5 id="collision-number-and-frequent-object">Collision Number and Frequent Object</h5><p>A data object is called frequence if its collision number #collision(o) is greater than or equal to a pre-specified collision threshold l.</p><h5 id="lsh-functions-for-c2lsh">LSH Functions for C2LSH</h5><p>Level-1: <span class="math inline">\(h(o)=\lfloor\frac{\vec a \cdot \vec o + b^*}{W}\rfloor\)</span>, it is <span class="math inline">\((1, c, p_1, p_2)\)</span> sensitive.</p><p>Level-R hash function: <span class="math inline">\(h^R(o)=\lfloor\frac{h(o)}{R}\rfloor\)</span>, it is <span class="math inline">\((R, cR, p_1, p_2)\)</span>-sensitive</p><p>An objects o's level-R bucket identified by the level-R bid, consists of R consecutive level-1 buckets identified by the level-1 bids.</p><h5 id="c2lsh-for-r-c-nn-search">C2LSH for <span class="math inline">\((R, c)\)</span>-NN Search</h5><p>Fristly calculate the buckets that <span class="math inline">\(q\)</span> falls in by <span class="math inline">\(h_i(q), i=1,2,...,m\)</span>, and find the objects collides with <span class="math inline">\(q\)</span>.</p><p>Then we compute #collides(o) for every <span class="math inline">\(o\)</span> and hence identify the set <span class="math inline">\(C\)</span> of all frequent objects. Then we compute <span class="math inline">\(max(\#C, \beta n)\)</span> frequent members of C. <span class="math inline">\(n\)</span> is the the cardinality of the database.</p><p>Collison threshold: <span class="math inline">\(l=\alpha m\)</span></p><p>These two properties should hold to ensure C2LSH correct:</p><p><span class="math inline">\(\mathcal{P}_1\)</span>: If there exists a data object o, s.t. <span class="math inline">\(o\in B(q, R)\)</span>, then o's collison number is at least l.</p><p><span class="math inline">\(\mathcal{P}_2\)</span>: The total number of false positives is less than <span class="math inline">\(\beta n​\)</span></p><figure><img src="/Algorithm/locality_sensitive_hash/image-20190526160936851.png" alt="image-20190526160936851"><figcaption>image-20190526160936851</figcaption></figure><h5 id="c2lsh-for-c-approxinate-nn-search">C2LSH for c-Approxinate NN Search</h5><h6 id="virtual-rehashing">Virtual Rehashing</h6><p>Given a query, object Q, there may not exist any data object within the ball centered at q with the radius <span class="math inline">\(R=1\)</span>. C2LSH then simulates the search of E2LSH at <span class="math inline">\(1, c, c^2, ....\)</span></p><p>According to observation above, locating the level-c bucket is equivalent to locating c level-1 buckets.</p><h6 id="neatest-neighbor-algorithm">Neatest Neighbor Algorithm</h6><figure><img src="/Algorithm/locality_sensitive_hash/image-20190526162845491.png" alt="image-20190526162845491"><figcaption>image-20190526162845491</figcaption></figure><h4 id="bayesian-lsh">Bayesian LSH</h4><h4 id="fast-lsh">Fast LSH</h4><h4 id="bi-level-lsh">Bi-Level LSH</h4><h3 id="sortingkeys-lsh">SortingKeys-LSH</h3><h3 id="analysis-and-modeling">Analysis and Modeling</h3><section class="footnotes" role="doc-endnotes"><hr><ol><li id="fn1" role="doc-endnote"><p>Localitysensitive hashing scheme based on p-stable distributions.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li><li id="fn2" role="doc-endnote"><p>Q. Lv, W. Josephson, Z. Wang, M. Charikar, and K. Li. Multiprobe lsh: Efﬁcient indexing for high-dimensional similarity search. In VLDB, pages 950–961, 2007. 3, 8<a href="#fnref2" class="footnote-back" role="doc-backlink">↩</a></p></li><li id="fn3" role="doc-endnote"><p>Gan, Junhao, et al. "Locality-sensitive hashing scheme based on dynamic collision counting." <em>Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data</em>. ACM, 2012.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩</a></p></li></ol></section></div><footer class="article-footer"></footer></div></article><article id="post-configuration/hexo_config" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/configuration/">configuration</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/configuration/hexo_config/"><time datetime="2018-11-17T04:12:59.000Z" itemprop="datePublished">2018-11-17</time></a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/raw/master/source/_posts/configuration/hexo_config.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/edit/master/source/_posts/configuration/hexo_config.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/lizeyan/blog/commits/master/source/_posts/configuration/hexo_config.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/configuration/hexo_config/">Hexo Config</a></h1></header><div class="article-entry" itemprop="articleBody"><p></p><h1 id="hexo-config">Hexo Config</h1><p></p></div><div class="article-more-link"><a href="/configuration/hexo_config/#more">Read More</a></div><footer class="article-footer"></footer></div></article><nav id="page-nav"><a class="extend prev" rel="prev" href="/index.md/page/3/">&laquo; Prev</a><a class="page-number" href="/index.md/">1</a><a class="page-number" href="/index.md/page/2/">2</a><a class="page-number" href="/index.md/page/3/">3</a><span class="page-number current">4</span></nav></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner">Zeyan LI &copy; 2020 <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png"></a><br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a></div></div></footer><script src="/libs/lightgallery/js/lightgallery.min.js"></script><script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script><script src="/libs/lightgallery/js/lg-pager.min.js"></script><script src="/libs/lightgallery/js/lg-autoplay.min.js"></script><script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script><script src="/libs/lightgallery/js/lg-zoom.min.js"></script><script src="/libs/lightgallery/js/lg-hash.min.js"></script><script src="/libs/lightgallery/js/lg-share.min.js"></script><script src="/libs/lightgallery/js/lg-video.min.js"></script><script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script async src="//cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/main.js"></script></div></body>